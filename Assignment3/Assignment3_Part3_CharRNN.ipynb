{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 3: Language Modeling with CharRNN\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sang-gil Lee, October 2018\n",
    "\n",
    "This is a character-level language model using recurrent neural networks (RNNs).\n",
    "It has become very popular as a starter kit for learning how RNN works in practice.\n",
    "\n",
    "Before we start, what is \"language modeling\" anyway? Intuitively, \"language modeling\" is teaching the model about a general probability distribution of our words and sentences.\n",
    "\n",
    "So we ask the model like: \"hey just say whatever words from your estimation of the wikipedia word distribution\", and the model responds like \"ok, i learned from wikipedia, and the most frequent word is \"the\". so let me start with \"the\". the wikipedia is blah blah blah\"\n",
    "\n",
    "Thus, by teaching the model to speak for itself, we can test the model's capability of learning temporal relationships between sequences.\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/karpathy/char-rnn\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "But the original code is written in lua torch which looks less pretty :(\n",
    "\n",
    "There is a clean port of char-RNN in TensorFlow\n",
    "https://github.com/sherjilozair/char-rnn-tensorflow\n",
    "This iPython notebook is basically a copypasta of this repo.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "### AND MOST IMPORTANTLY, IF YOU JUST BLINDLY COPY PASTE THE CODE, YOU SHALL RUIN YOUR EXAM.\n",
    "### The exam is designed to be solvable for students that actually have written the code themselves.\n",
    "At least strictly re-type the codes from the original repo line-by-line, and understand what each line means thoroughly.\n",
    "\n",
    "## YOU HAVE BEEN WARNED. :)\n",
    "\n",
    "\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1, 2 & 3**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Character language modeling (40 points)\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. implementing **1. \\_\\_init\\_\\_()** and **2. sample()** of RNN **Model()** class from **char_rnn.py**\n",
    "\n",
    "2. briefly summarizing, at the end of the script, how you implmeneted the model & why you changed some other parts of the code. yes,  <font color=red> there are other intentional pitfalls inside the code </font>. just copy-pasting the \\_\\_init\\_\\_() will not work. can you tell me why?\n",
    "\n",
    "### The Definition of **\"work\"** is as follows:\n",
    "\n",
    "1. Training loss must be <font color=red> below 0.2 </font>. We will check the training loss output from the training code block. We don't split the data into train-valid-test. Don't forget to <font color=red> NOT clear the output from train(args)</font>, where the training loss will be printed! TA will check the logged output from train(args)\n",
    "\n",
    "2. calling sample(args.sample) at the last code block <font color=red> must generate some meaningful sentences </font>. The quality of the sentence does not count, unless the generated sentence is something like \"aaaaaaaaaaaaaabbbbbb\" or \"b\" u tlttfcwaU c  fGcnrh i.\\nh mt he!bsthpme\".\n",
    "\n",
    "\n",
    "\n",
    "Now proceed to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# ipython magic function for limiting the gpu to be seen for tensorflow\n",
    "# if you have just 1 GPU, specify the value to 0\n",
    "# if you have multiple GPUs (nut) and want to specify which GPU to use, specify this value to 0 or 1 or etc.\n",
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TensorFlow vram efficiency: if this is not specified, the model hogs all the VRAM even if it's not necessary\n",
    "# bad & greedy TF! but it has a reason for this design choice FWIW, try googling it if interested\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=128, data_dir='data/tinyshakespeare', decay_rate=0.5, grad_clip=5.0, init_from=None, input_keep_prob=0.1, learning_rate=0.001, model='lstm', num_epochs=4000, num_layers=1, output_keep_prob=0.1, rnn_size=638, save_dir='models_char_rnn', save_every=1000, seq_length=500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing\n",
    "parser = argparse.ArgumentParser(\n",
    "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',\n",
    "                    help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='directory to store checkpointed models')\n",
    "parser.add_argument('--save_every', type=int, default=1000,\n",
    "                    help='Save frequency. Number of passes between checkpoints of the model.')\n",
    "parser.add_argument('--init_from', type=str, default=None,\n",
    "                    help=\"\"\"continue training from saved model at this path (usually \"save\").\n",
    "                        Path must contain files saved by previous training process:\n",
    "                        'config.pkl'        : configuration;\n",
    "                        'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                        'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                              Note: this file contains absolute paths, be careful when moving files around;\n",
    "                        'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                         Model params must be the same between multiple runs (model, rnn_size, num_layers and seq_length).\n",
    "                    \"\"\")\n",
    "# Model params\n",
    "parser.add_argument('--model', type=str, default='lstm',\n",
    "                    help='lstm, rnn, gru, or nas')\n",
    "parser.add_argument('--rnn_size', type=int, default=638,\n",
    "                    help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=1,\n",
    "                    help='number of layers in the RNN')\n",
    "# Optimization\n",
    "parser.add_argument('--seq_length', type=int, default=500,\n",
    "                    help='RNN sequence length. Number of timesteps to unroll for.')\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help=\"\"\"minibatch size. Number of sequences propagated through the network in parallel.\n",
    "                            Pick batch-sizes to fully leverage the GPU (e.g. until the memory is filled up)\n",
    "                            commonly in the range 10-500.\"\"\")\n",
    "parser.add_argument('--num_epochs', type=int, default=4000,\n",
    "                    help='number of epochs. Number of full passes through the training examples.')\n",
    "parser.add_argument('--grad_clip', type=float, default=5.,\n",
    "                    help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.5,\n",
    "                    help='decay rate for rmsprop')\n",
    "parser.add_argument('--output_keep_prob', type=float, default=0.1,\n",
    "                    help='probability of keeping weights in the hidden layer')\n",
    "parser.add_argument('--input_keep_prob', type=float, default=0.1,\n",
    "                    help='probability of keeping weights in the input layer')\n",
    "\n",
    "# needed for argparsing within jupyter notebook\n",
    "# https://stackoverflow.com/questions/30656777/how-to-call-module-written-with-argparse-in-ipython-notebook\n",
    "sys.argv = ['-f']\n",
    "args = parser.parse_args()\n",
    "\n",
    "# print args: see if the hyperparemeters look pretty to you\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "[[49  9  7 ...  3  3  7]\n",
      " [40 35 24 ...  2  5  3]\n",
      " [ 4  7  1 ...  0  4  8]\n",
      " ...\n",
      " [ 0 21  0 ... 10 52 26]\n",
      " [ 9  2  5 ...  5  1  0]\n",
      " [ 0  6 17 ...  3  0 22]]\n",
      "(128, 500)\n",
      "[[ 9  7  6 ...  3  7  0]\n",
      " [35 24 10 ...  5  3 13]\n",
      " [ 7  1  0 ...  4  8 12]\n",
      " ...\n",
      " [21  0 11 ... 52 26 29]\n",
      " [ 2  5  3 ...  1  0  6]\n",
      " [ 6 17  4 ...  0 22  1]]\n",
      "(128, 500)\n"
     ]
    }
   ],
   "source": [
    "# protip: always check the data and poke around the data yourself\n",
    "# you will get a lot of insights by looking at the data\n",
    "data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "data_loader.reset_batch_pointer()\n",
    "\n",
    "x, y = data_loader.next_batch()\n",
    "\n",
    "# our data has a shape of (N, T), where N=batch_size and T=seq_length\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1\n",
      "  0 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1\n",
      "  4  7  0 14  1  0  6 23  1  4 28 25 10 10 26 11 11 24 10 35 23  1  4 28\n",
      " 16  0  6 23  1  4 28 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24\n",
      " 10 50  3 13  0  4  7  1  0  4 11 11  0  7  1  6  3 11 27  1 12  0  7  4\n",
      "  2  5  1  7  0  2  3  0 12  9  1  0  2  5  4  8  0  2  3  0 18  4 14  9\n",
      "  6  5 44 10 10 26 11 11 24 10 34  1  6  3 11 27  1 12 25  0  7  1  6  3\n",
      " 11 27  1 12 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 49  9\n",
      "  7  6  2 16  0 15  3 13  0 28  8  3 17  0 37  4  9 13  6  0 42  4  7 19\n",
      "  9 13  6  0  9  6  0 19  5  9  1 18  0  1  8  1 14 15  0  2  3  0  2  5\n",
      "  1  0 23  1  3 23 11  1 25 10 10 26 11 11 24 10 39  1  0 28  8  3 17 30\n",
      "  2 16  0 17  1  0 28  8  3 17 30  2 25 10 10 49  9  7  6  2  0 37  9  2\n",
      "  9 57  1  8 24 10 36  1  2  0 13  6  0 28  9 11 11  0  5  9 14 16  0  4\n",
      "  8 12  0 17  1 30 11 11  0  5  4 27  1  0 19  3  7  8  0  4  2  0  3 13\n",
      "  7  0  3 17  8  0 23  7  9 19  1 25 10 21  6 30  2  0  4  0 27  1  7 12\n",
      "  9 19  2 44 10 10 26 11 11 24 10 33  3  0 14  3  7  1  0  2  4 11 28  9\n",
      "  8 20  0  3  8 30  2 38  0 11  1  2  0  9  2  0 22  1  0 12  3  8  1 24\n",
      "  0  4 17  4 15 16  0  4 17  4 15 46 10 10 35  1 19  3  8 12  0 37  9  2\n",
      "  9 57  1  8 24 10 32  8  1  0 17  3  7 12 16  0 20  3  3 12  0 19  9  2\n",
      "  9 57  1  8  6 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 39\n",
      "  1  0  4  7  1  0  4 19 19  3 13  8  2  1 12  0 23  3  3  7]\n",
      "[ 9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1  0\n",
      " 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1  4\n",
      "  7  0 14  1  0  6 23  1  4 28 25 10 10 26 11 11 24 10 35 23  1  4 28 16\n",
      "  0  6 23  1  4 28 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10\n",
      " 50  3 13  0  4  7  1  0  4 11 11  0  7  1  6  3 11 27  1 12  0  7  4  2\n",
      "  5  1  7  0  2  3  0 12  9  1  0  2  5  4  8  0  2  3  0 18  4 14  9  6\n",
      "  5 44 10 10 26 11 11 24 10 34  1  6  3 11 27  1 12 25  0  7  1  6  3 11\n",
      " 27  1 12 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 49  9  7\n",
      "  6  2 16  0 15  3 13  0 28  8  3 17  0 37  4  9 13  6  0 42  4  7 19  9\n",
      " 13  6  0  9  6  0 19  5  9  1 18  0  1  8  1 14 15  0  2  3  0  2  5  1\n",
      "  0 23  1  3 23 11  1 25 10 10 26 11 11 24 10 39  1  0 28  8  3 17 30  2\n",
      " 16  0 17  1  0 28  8  3 17 30  2 25 10 10 49  9  7  6  2  0 37  9  2  9\n",
      " 57  1  8 24 10 36  1  2  0 13  6  0 28  9 11 11  0  5  9 14 16  0  4  8\n",
      " 12  0 17  1 30 11 11  0  5  4 27  1  0 19  3  7  8  0  4  2  0  3 13  7\n",
      "  0  3 17  8  0 23  7  9 19  1 25 10 21  6 30  2  0  4  0 27  1  7 12  9\n",
      " 19  2 44 10 10 26 11 11 24 10 33  3  0 14  3  7  1  0  2  4 11 28  9  8\n",
      " 20  0  3  8 30  2 38  0 11  1  2  0  9  2  0 22  1  0 12  3  8  1 24  0\n",
      "  4 17  4 15 16  0  4 17  4 15 46 10 10 35  1 19  3  8 12  0 37  9  2  9\n",
      " 57  1  8 24 10 32  8  1  0 17  3  7 12 16  0 20  3  3 12  0 19  9  2  9\n",
      " 57  1  8  6 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 39  1\n",
      "  0  4  7  1  0  4 19 19  3 13  8  2  1 12  0 23  3  3  7  0]\n"
     ]
    }
   ],
   "source": [
    "# see what the first entry of the batch look like\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "# y is just an x shifted to the left by one: so the network will predict the next token y given x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop definition\n",
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    print(\"vocabulary size: \" + str(args.vocab_size))\n",
    "\n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        # check if all necessary files exist\n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.latest_checkpoint(args.init_from)\n",
    "        assert ckpt, \"No checkpoint found\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same = [\"model\", \"rnn_size\", \"num_layers\", \"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "\n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagree on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagree on dictionary mappings!\"\n",
    "\n",
    "    if not os.path.isdir(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "    \n",
    "    print(\"building the model... may take some time...\")\n",
    "    ##################### This line builds the CharRNN model defined in char_rnn.py #####################\n",
    "    tf.reset_default_graph()\n",
    "    model = Model(args)\n",
    "    print(\"model built! starting training...\")\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt)\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr,\n",
    "                               args.learning_rate * (args.decay_rate ** (e/args.num_epochs))))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state)\n",
    "            \n",
    "            for b in range(int(data_loader.num_batches)):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                for i, (c, h) in enumerate(model.initial_state):\n",
    "                    feed[c] = state[i].c\n",
    "                    feed[h] = state[i].h\n",
    "\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "\n",
    "                end = time.time()\n",
    "                \n",
    "                # print training log every 100 steps\n",
    "                if ((e * data_loader.num_batches + b) % 100 == 0):\n",
    "                    print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                          .format(e * data_loader.num_batches + b,\n",
    "                                  args.num_epochs * data_loader.num_batches,\n",
    "                                  e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                        or (e == args.num_epochs-1 and\n",
    "                            b == data_loader.num_batches-1):\n",
    "                    # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path,\n",
    "                               global_step=e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "building the model... may take some time...\n",
      "model built! starting training...\n",
      "0/68000 (epoch 0), train_loss = 4.263, time/batch = 2.110\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "100/68000 (epoch 5), train_loss = 2.907, time/batch = 0.302\n",
      "200/68000 (epoch 11), train_loss = 2.649, time/batch = 0.301\n",
      "300/68000 (epoch 17), train_loss = 2.547, time/batch = 0.302\n",
      "400/68000 (epoch 23), train_loss = 2.474, time/batch = 0.301\n",
      "500/68000 (epoch 29), train_loss = 2.434, time/batch = 0.306\n",
      "600/68000 (epoch 35), train_loss = 2.402, time/batch = 0.305\n",
      "700/68000 (epoch 41), train_loss = 2.369, time/batch = 0.305\n",
      "800/68000 (epoch 47), train_loss = 2.344, time/batch = 0.301\n",
      "900/68000 (epoch 52), train_loss = 2.327, time/batch = 0.302\n",
      "1000/68000 (epoch 58), train_loss = 2.296, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "1100/68000 (epoch 64), train_loss = 2.279, time/batch = 0.304\n",
      "1200/68000 (epoch 70), train_loss = 2.256, time/batch = 0.304\n",
      "1300/68000 (epoch 76), train_loss = 2.231, time/batch = 0.306\n",
      "1400/68000 (epoch 82), train_loss = 2.228, time/batch = 0.305\n",
      "1500/68000 (epoch 88), train_loss = 2.224, time/batch = 0.304\n",
      "1600/68000 (epoch 94), train_loss = 2.207, time/batch = 0.304\n",
      "1700/68000 (epoch 100), train_loss = 2.207, time/batch = 0.301\n",
      "1800/68000 (epoch 105), train_loss = 2.180, time/batch = 0.307\n",
      "1900/68000 (epoch 111), train_loss = 2.172, time/batch = 0.308\n",
      "2000/68000 (epoch 117), train_loss = 2.155, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "2100/68000 (epoch 123), train_loss = 2.136, time/batch = 0.304\n",
      "2200/68000 (epoch 129), train_loss = 2.129, time/batch = 0.303\n",
      "2300/68000 (epoch 135), train_loss = 2.128, time/batch = 0.307\n",
      "2400/68000 (epoch 141), train_loss = 2.108, time/batch = 0.307\n",
      "2500/68000 (epoch 147), train_loss = 2.104, time/batch = 0.305\n",
      "2600/68000 (epoch 152), train_loss = 2.105, time/batch = 0.304\n",
      "2700/68000 (epoch 158), train_loss = 2.093, time/batch = 0.305\n",
      "2800/68000 (epoch 164), train_loss = 2.093, time/batch = 0.304\n",
      "2900/68000 (epoch 170), train_loss = 2.067, time/batch = 0.303\n",
      "3000/68000 (epoch 176), train_loss = 2.063, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "3100/68000 (epoch 182), train_loss = 2.061, time/batch = 0.306\n",
      "3200/68000 (epoch 188), train_loss = 2.072, time/batch = 0.304\n",
      "3300/68000 (epoch 194), train_loss = 2.064, time/batch = 0.306\n",
      "3400/68000 (epoch 200), train_loss = 2.066, time/batch = 0.302\n",
      "3500/68000 (epoch 205), train_loss = 2.043, time/batch = 0.302\n",
      "3600/68000 (epoch 211), train_loss = 2.042, time/batch = 0.307\n",
      "3700/68000 (epoch 217), train_loss = 2.040, time/batch = 0.308\n",
      "3800/68000 (epoch 223), train_loss = 2.023, time/batch = 0.308\n",
      "3900/68000 (epoch 229), train_loss = 2.018, time/batch = 0.303\n",
      "4000/68000 (epoch 235), train_loss = 2.022, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "4100/68000 (epoch 241), train_loss = 2.007, time/batch = 0.306\n",
      "4200/68000 (epoch 247), train_loss = 2.012, time/batch = 0.306\n",
      "4300/68000 (epoch 252), train_loss = 2.005, time/batch = 0.306\n",
      "4400/68000 (epoch 258), train_loss = 2.005, time/batch = 0.303\n",
      "4500/68000 (epoch 264), train_loss = 2.005, time/batch = 0.307\n",
      "4600/68000 (epoch 270), train_loss = 1.984, time/batch = 0.307\n",
      "4700/68000 (epoch 276), train_loss = 1.979, time/batch = 0.305\n",
      "4800/68000 (epoch 282), train_loss = 1.988, time/batch = 0.305\n",
      "4900/68000 (epoch 288), train_loss = 1.997, time/batch = 0.305\n",
      "5000/68000 (epoch 294), train_loss = 1.991, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "5100/68000 (epoch 300), train_loss = 1.998, time/batch = 0.301\n",
      "5200/68000 (epoch 305), train_loss = 1.974, time/batch = 0.304\n",
      "5300/68000 (epoch 311), train_loss = 1.970, time/batch = 0.305\n",
      "5400/68000 (epoch 317), train_loss = 1.972, time/batch = 0.308\n",
      "5500/68000 (epoch 323), train_loss = 1.963, time/batch = 0.304\n",
      "5600/68000 (epoch 329), train_loss = 1.954, time/batch = 0.306\n",
      "5700/68000 (epoch 335), train_loss = 1.963, time/batch = 0.304\n",
      "5800/68000 (epoch 341), train_loss = 1.952, time/batch = 0.302\n",
      "5900/68000 (epoch 347), train_loss = 1.958, time/batch = 0.306\n",
      "6000/68000 (epoch 352), train_loss = 1.955, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "6100/68000 (epoch 358), train_loss = 1.944, time/batch = 0.306\n",
      "6200/68000 (epoch 364), train_loss = 1.944, time/batch = 0.303\n",
      "6300/68000 (epoch 370), train_loss = 1.931, time/batch = 0.307\n",
      "6400/68000 (epoch 376), train_loss = 1.929, time/batch = 0.306\n",
      "6500/68000 (epoch 382), train_loss = 1.933, time/batch = 0.307\n",
      "6600/68000 (epoch 388), train_loss = 1.952, time/batch = 0.307\n",
      "6700/68000 (epoch 394), train_loss = 1.947, time/batch = 0.306\n",
      "6800/68000 (epoch 400), train_loss = 1.950, time/batch = 0.301\n",
      "6900/68000 (epoch 405), train_loss = 1.934, time/batch = 0.307\n",
      "7000/68000 (epoch 411), train_loss = 1.933, time/batch = 0.308\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "7100/68000 (epoch 417), train_loss = 1.937, time/batch = 0.307\n",
      "7200/68000 (epoch 423), train_loss = 1.920, time/batch = 0.302\n",
      "7300/68000 (epoch 429), train_loss = 1.916, time/batch = 0.304\n",
      "7400/68000 (epoch 435), train_loss = 1.918, time/batch = 0.307\n",
      "7500/68000 (epoch 441), train_loss = 1.917, time/batch = 0.303\n",
      "7600/68000 (epoch 447), train_loss = 1.915, time/batch = 0.306\n",
      "7700/68000 (epoch 452), train_loss = 1.913, time/batch = 0.303\n",
      "7800/68000 (epoch 458), train_loss = 1.912, time/batch = 0.311\n",
      "7900/68000 (epoch 464), train_loss = 1.907, time/batch = 0.311\n",
      "8000/68000 (epoch 470), train_loss = 1.894, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "8100/68000 (epoch 476), train_loss = 1.900, time/batch = 0.306\n",
      "8200/68000 (epoch 482), train_loss = 1.907, time/batch = 0.307\n",
      "8300/68000 (epoch 488), train_loss = 1.919, time/batch = 0.308\n",
      "8400/68000 (epoch 494), train_loss = 1.914, time/batch = 0.303\n",
      "8500/68000 (epoch 500), train_loss = 1.926, time/batch = 0.303\n",
      "8600/68000 (epoch 505), train_loss = 1.900, time/batch = 0.306\n",
      "8700/68000 (epoch 511), train_loss = 1.906, time/batch = 0.310\n",
      "8800/68000 (epoch 517), train_loss = 1.904, time/batch = 0.308\n",
      "8900/68000 (epoch 523), train_loss = 1.889, time/batch = 0.305\n",
      "9000/68000 (epoch 529), train_loss = 1.887, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "9100/68000 (epoch 535), train_loss = 1.924, time/batch = 0.309\n",
      "9200/68000 (epoch 541), train_loss = 1.891, time/batch = 0.310\n",
      "9300/68000 (epoch 547), train_loss = 1.898, time/batch = 0.303\n",
      "9400/68000 (epoch 552), train_loss = 1.888, time/batch = 0.303\n",
      "9500/68000 (epoch 558), train_loss = 1.888, time/batch = 0.303\n",
      "9600/68000 (epoch 564), train_loss = 1.883, time/batch = 0.309\n",
      "9700/68000 (epoch 570), train_loss = 1.874, time/batch = 0.306\n",
      "9800/68000 (epoch 576), train_loss = 1.866, time/batch = 0.306\n",
      "9900/68000 (epoch 582), train_loss = 1.882, time/batch = 0.308\n",
      "10000/68000 (epoch 588), train_loss = 1.890, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "10100/68000 (epoch 594), train_loss = 1.891, time/batch = 0.308\n",
      "10200/68000 (epoch 600), train_loss = 1.907, time/batch = 0.304\n",
      "10300/68000 (epoch 605), train_loss = 1.885, time/batch = 0.304\n",
      "10400/68000 (epoch 611), train_loss = 1.886, time/batch = 0.310\n",
      "10500/68000 (epoch 617), train_loss = 1.876, time/batch = 0.304\n",
      "10600/68000 (epoch 623), train_loss = 1.866, time/batch = 0.309\n",
      "10700/68000 (epoch 629), train_loss = 1.878, time/batch = 0.305\n",
      "10800/68000 (epoch 635), train_loss = 1.868, time/batch = 0.306\n",
      "10900/68000 (epoch 641), train_loss = 1.868, time/batch = 0.307\n",
      "11000/68000 (epoch 647), train_loss = 1.862, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "11100/68000 (epoch 652), train_loss = 1.874, time/batch = 0.303\n",
      "11200/68000 (epoch 658), train_loss = 1.877, time/batch = 0.303\n",
      "11300/68000 (epoch 664), train_loss = 1.867, time/batch = 0.303\n",
      "11400/68000 (epoch 670), train_loss = 1.859, time/batch = 0.305\n",
      "11500/68000 (epoch 676), train_loss = 1.852, time/batch = 0.305\n",
      "11600/68000 (epoch 682), train_loss = 1.862, time/batch = 0.306\n",
      "11700/68000 (epoch 688), train_loss = 1.882, time/batch = 0.304\n",
      "11800/68000 (epoch 694), train_loss = 1.880, time/batch = 0.305\n",
      "11900/68000 (epoch 700), train_loss = 1.889, time/batch = 0.302\n",
      "12000/68000 (epoch 705), train_loss = 1.860, time/batch = 0.303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to models_char_rnn/model.ckpt\n",
      "12100/68000 (epoch 711), train_loss = 1.870, time/batch = 0.307\n",
      "12200/68000 (epoch 717), train_loss = 1.866, time/batch = 0.306\n",
      "12300/68000 (epoch 723), train_loss = 1.849, time/batch = 0.306\n",
      "12400/68000 (epoch 729), train_loss = 1.855, time/batch = 0.308\n",
      "12500/68000 (epoch 735), train_loss = 1.858, time/batch = 0.303\n",
      "12600/68000 (epoch 741), train_loss = 1.850, time/batch = 0.308\n",
      "12700/68000 (epoch 747), train_loss = 1.849, time/batch = 0.303\n",
      "12800/68000 (epoch 752), train_loss = 1.853, time/batch = 0.307\n",
      "12900/68000 (epoch 758), train_loss = 1.855, time/batch = 0.308\n",
      "13000/68000 (epoch 764), train_loss = 1.851, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "13100/68000 (epoch 770), train_loss = 1.843, time/batch = 0.308\n",
      "13200/68000 (epoch 776), train_loss = 1.838, time/batch = 0.307\n",
      "13300/68000 (epoch 782), train_loss = 1.844, time/batch = 0.304\n",
      "13400/68000 (epoch 788), train_loss = 1.858, time/batch = 0.310\n",
      "13500/68000 (epoch 794), train_loss = 1.861, time/batch = 0.305\n",
      "13600/68000 (epoch 800), train_loss = 1.871, time/batch = 0.307\n",
      "13700/68000 (epoch 805), train_loss = 1.848, time/batch = 0.303\n",
      "13800/68000 (epoch 811), train_loss = 1.858, time/batch = 0.306\n",
      "13900/68000 (epoch 817), train_loss = 1.849, time/batch = 0.305\n",
      "14000/68000 (epoch 823), train_loss = 1.837, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "14100/68000 (epoch 829), train_loss = 1.836, time/batch = 0.306\n",
      "14200/68000 (epoch 835), train_loss = 1.851, time/batch = 0.307\n",
      "14300/68000 (epoch 841), train_loss = 1.830, time/batch = 0.306\n",
      "14400/68000 (epoch 847), train_loss = 1.842, time/batch = 0.307\n",
      "14500/68000 (epoch 852), train_loss = 1.844, time/batch = 0.305\n",
      "14600/68000 (epoch 858), train_loss = 1.847, time/batch = 0.307\n",
      "14700/68000 (epoch 864), train_loss = 1.837, time/batch = 0.306\n",
      "14800/68000 (epoch 870), train_loss = 1.830, time/batch = 0.307\n",
      "14900/68000 (epoch 876), train_loss = 1.825, time/batch = 0.305\n",
      "15000/68000 (epoch 882), train_loss = 1.837, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "15100/68000 (epoch 888), train_loss = 1.855, time/batch = 0.302\n",
      "15200/68000 (epoch 894), train_loss = 1.856, time/batch = 0.306\n",
      "15300/68000 (epoch 900), train_loss = 1.865, time/batch = 0.301\n",
      "15400/68000 (epoch 905), train_loss = 1.839, time/batch = 0.307\n",
      "15500/68000 (epoch 911), train_loss = 1.845, time/batch = 0.306\n",
      "15600/68000 (epoch 917), train_loss = 1.837, time/batch = 0.306\n",
      "15700/68000 (epoch 923), train_loss = 1.828, time/batch = 0.307\n",
      "15800/68000 (epoch 929), train_loss = 1.831, time/batch = 0.304\n",
      "15900/68000 (epoch 935), train_loss = 1.839, time/batch = 0.307\n",
      "16000/68000 (epoch 941), train_loss = 1.829, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "16100/68000 (epoch 947), train_loss = 1.828, time/batch = 0.303\n",
      "16200/68000 (epoch 952), train_loss = 1.839, time/batch = 0.307\n",
      "16300/68000 (epoch 958), train_loss = 1.835, time/batch = 0.307\n",
      "16400/68000 (epoch 964), train_loss = 1.830, time/batch = 0.307\n",
      "16500/68000 (epoch 970), train_loss = 1.819, time/batch = 0.307\n",
      "16600/68000 (epoch 976), train_loss = 1.814, time/batch = 0.303\n",
      "16700/68000 (epoch 982), train_loss = 1.827, time/batch = 0.306\n",
      "16800/68000 (epoch 988), train_loss = 1.833, time/batch = 0.308\n",
      "16900/68000 (epoch 994), train_loss = 1.839, time/batch = 0.306\n",
      "17000/68000 (epoch 1000), train_loss = 1.852, time/batch = 0.309\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "17100/68000 (epoch 1005), train_loss = 1.825, time/batch = 0.303\n",
      "17200/68000 (epoch 1011), train_loss = 1.838, time/batch = 0.306\n",
      "17300/68000 (epoch 1017), train_loss = 1.827, time/batch = 0.305\n",
      "17400/68000 (epoch 1023), train_loss = 1.818, time/batch = 0.308\n",
      "17500/68000 (epoch 1029), train_loss = 1.822, time/batch = 0.304\n",
      "17600/68000 (epoch 1035), train_loss = 1.825, time/batch = 0.303\n",
      "17700/68000 (epoch 1041), train_loss = 1.822, time/batch = 0.303\n",
      "17800/68000 (epoch 1047), train_loss = 1.818, time/batch = 0.306\n",
      "17900/68000 (epoch 1052), train_loss = 1.823, time/batch = 0.307\n",
      "18000/68000 (epoch 1058), train_loss = 1.814, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "18100/68000 (epoch 1064), train_loss = 1.822, time/batch = 0.303\n",
      "18200/68000 (epoch 1070), train_loss = 1.812, time/batch = 0.303\n",
      "18300/68000 (epoch 1076), train_loss = 1.806, time/batch = 0.306\n",
      "18400/68000 (epoch 1082), train_loss = 1.817, time/batch = 0.304\n",
      "18500/68000 (epoch 1088), train_loss = 1.835, time/batch = 0.306\n",
      "18600/68000 (epoch 1094), train_loss = 1.828, time/batch = 0.307\n",
      "18700/68000 (epoch 1100), train_loss = 1.840, time/batch = 0.303\n",
      "18800/68000 (epoch 1105), train_loss = 1.815, time/batch = 0.307\n",
      "18900/68000 (epoch 1111), train_loss = 1.821, time/batch = 0.304\n",
      "19000/68000 (epoch 1117), train_loss = 1.825, time/batch = 0.310\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "19100/68000 (epoch 1123), train_loss = 1.816, time/batch = 0.303\n",
      "19200/68000 (epoch 1129), train_loss = 1.818, time/batch = 0.305\n",
      "19300/68000 (epoch 1135), train_loss = 1.816, time/batch = 0.303\n",
      "19400/68000 (epoch 1141), train_loss = 1.805, time/batch = 0.305\n",
      "19500/68000 (epoch 1147), train_loss = 1.814, time/batch = 0.306\n",
      "19600/68000 (epoch 1152), train_loss = 1.809, time/batch = 0.306\n",
      "19700/68000 (epoch 1158), train_loss = 1.819, time/batch = 0.305\n",
      "19800/68000 (epoch 1164), train_loss = 1.816, time/batch = 0.306\n",
      "19900/68000 (epoch 1170), train_loss = 1.803, time/batch = 0.305\n",
      "20000/68000 (epoch 1176), train_loss = 1.795, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "20100/68000 (epoch 1182), train_loss = 1.799, time/batch = 0.308\n",
      "20200/68000 (epoch 1188), train_loss = 1.823, time/batch = 0.306\n",
      "20300/68000 (epoch 1194), train_loss = 1.825, time/batch = 0.304\n",
      "20400/68000 (epoch 1200), train_loss = 1.838, time/batch = 0.302\n",
      "20500/68000 (epoch 1205), train_loss = 1.816, time/batch = 0.303\n",
      "20600/68000 (epoch 1211), train_loss = 1.805, time/batch = 0.307\n",
      "20700/68000 (epoch 1217), train_loss = 1.812, time/batch = 0.305\n",
      "20800/68000 (epoch 1223), train_loss = 1.802, time/batch = 0.308\n",
      "20900/68000 (epoch 1229), train_loss = 1.807, time/batch = 0.308\n",
      "21000/68000 (epoch 1235), train_loss = 1.805, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "21100/68000 (epoch 1241), train_loss = 1.810, time/batch = 0.305\n",
      "21200/68000 (epoch 1247), train_loss = 1.802, time/batch = 0.305\n",
      "21300/68000 (epoch 1252), train_loss = 1.809, time/batch = 0.307\n",
      "21400/68000 (epoch 1258), train_loss = 1.803, time/batch = 0.308\n",
      "21500/68000 (epoch 1264), train_loss = 1.802, time/batch = 0.306\n",
      "21600/68000 (epoch 1270), train_loss = 1.793, time/batch = 0.307\n",
      "21700/68000 (epoch 1276), train_loss = 1.796, time/batch = 0.307\n",
      "21800/68000 (epoch 1282), train_loss = 1.797, time/batch = 0.307\n",
      "21900/68000 (epoch 1288), train_loss = 1.814, time/batch = 0.307\n",
      "22000/68000 (epoch 1294), train_loss = 1.815, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "22100/68000 (epoch 1300), train_loss = 1.829, time/batch = 0.301\n",
      "22200/68000 (epoch 1305), train_loss = 1.815, time/batch = 0.306\n",
      "22300/68000 (epoch 1311), train_loss = 1.807, time/batch = 0.307\n",
      "22400/68000 (epoch 1317), train_loss = 1.815, time/batch = 0.307\n",
      "22500/68000 (epoch 1323), train_loss = 1.796, time/batch = 0.304\n",
      "22600/68000 (epoch 1329), train_loss = 1.798, time/batch = 0.303\n",
      "22700/68000 (epoch 1335), train_loss = 1.806, time/batch = 0.303\n",
      "22800/68000 (epoch 1341), train_loss = 1.799, time/batch = 0.307\n",
      "22900/68000 (epoch 1347), train_loss = 1.798, time/batch = 0.303\n",
      "23000/68000 (epoch 1352), train_loss = 1.804, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "23100/68000 (epoch 1358), train_loss = 1.801, time/batch = 0.305\n",
      "23200/68000 (epoch 1364), train_loss = 1.800, time/batch = 0.303\n",
      "23300/68000 (epoch 1370), train_loss = 1.784, time/batch = 0.306\n",
      "23400/68000 (epoch 1376), train_loss = 1.783, time/batch = 0.309\n",
      "23500/68000 (epoch 1382), train_loss = 1.800, time/batch = 0.304\n",
      "23600/68000 (epoch 1388), train_loss = 1.811, time/batch = 0.303\n",
      "23700/68000 (epoch 1394), train_loss = 1.810, time/batch = 0.308\n",
      "23800/68000 (epoch 1400), train_loss = 1.822, time/batch = 0.307\n",
      "23900/68000 (epoch 1405), train_loss = 1.798, time/batch = 0.308\n",
      "24000/68000 (epoch 1411), train_loss = 1.797, time/batch = 0.306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to models_char_rnn/model.ckpt\n",
      "24100/68000 (epoch 1417), train_loss = 1.804, time/batch = 0.303\n",
      "24200/68000 (epoch 1423), train_loss = 1.794, time/batch = 0.307\n",
      "24300/68000 (epoch 1429), train_loss = 1.788, time/batch = 0.306\n",
      "24400/68000 (epoch 1435), train_loss = 1.802, time/batch = 0.305\n",
      "24500/68000 (epoch 1441), train_loss = 1.802, time/batch = 0.305\n",
      "24600/68000 (epoch 1447), train_loss = 1.791, time/batch = 0.308\n",
      "24700/68000 (epoch 1452), train_loss = 1.803, time/batch = 0.305\n",
      "24800/68000 (epoch 1458), train_loss = 1.799, time/batch = 0.303\n",
      "24900/68000 (epoch 1464), train_loss = 1.794, time/batch = 0.306\n",
      "25000/68000 (epoch 1470), train_loss = 1.777, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "25100/68000 (epoch 1476), train_loss = 1.776, time/batch = 0.307\n",
      "25200/68000 (epoch 1482), train_loss = 1.795, time/batch = 0.306\n",
      "25300/68000 (epoch 1488), train_loss = 1.810, time/batch = 0.305\n",
      "25400/68000 (epoch 1494), train_loss = 1.806, time/batch = 0.304\n",
      "25500/68000 (epoch 1500), train_loss = 1.819, time/batch = 0.307\n",
      "25600/68000 (epoch 1505), train_loss = 1.800, time/batch = 0.306\n",
      "25700/68000 (epoch 1511), train_loss = 1.794, time/batch = 0.304\n",
      "25800/68000 (epoch 1517), train_loss = 1.792, time/batch = 0.303\n",
      "25900/68000 (epoch 1523), train_loss = 1.782, time/batch = 0.307\n",
      "26000/68000 (epoch 1529), train_loss = 1.789, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "26100/68000 (epoch 1535), train_loss = 1.792, time/batch = 0.304\n",
      "26200/68000 (epoch 1541), train_loss = 1.784, time/batch = 0.304\n",
      "26300/68000 (epoch 1547), train_loss = 1.783, time/batch = 0.302\n",
      "26400/68000 (epoch 1552), train_loss = 1.786, time/batch = 0.310\n",
      "26500/68000 (epoch 1558), train_loss = 1.795, time/batch = 0.305\n",
      "26600/68000 (epoch 1564), train_loss = 1.794, time/batch = 0.303\n",
      "26700/68000 (epoch 1570), train_loss = 1.779, time/batch = 0.307\n",
      "26800/68000 (epoch 1576), train_loss = 1.775, time/batch = 0.307\n",
      "26900/68000 (epoch 1582), train_loss = 1.778, time/batch = 0.307\n",
      "27000/68000 (epoch 1588), train_loss = 1.804, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "27100/68000 (epoch 1594), train_loss = 1.805, time/batch = 0.305\n",
      "27200/68000 (epoch 1600), train_loss = 1.819, time/batch = 0.301\n",
      "27300/68000 (epoch 1605), train_loss = 1.787, time/batch = 0.304\n",
      "27400/68000 (epoch 1611), train_loss = 1.792, time/batch = 0.306\n",
      "27500/68000 (epoch 1617), train_loss = 1.780, time/batch = 0.309\n",
      "27600/68000 (epoch 1623), train_loss = 1.781, time/batch = 0.304\n",
      "27700/68000 (epoch 1629), train_loss = 1.781, time/batch = 0.305\n",
      "27800/68000 (epoch 1635), train_loss = 1.786, time/batch = 0.302\n",
      "27900/68000 (epoch 1641), train_loss = 1.771, time/batch = 0.307\n",
      "28000/68000 (epoch 1647), train_loss = 1.782, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "28100/68000 (epoch 1652), train_loss = 1.779, time/batch = 0.306\n",
      "28200/68000 (epoch 1658), train_loss = 1.788, time/batch = 0.308\n",
      "28300/68000 (epoch 1664), train_loss = 1.783, time/batch = 0.305\n",
      "28400/68000 (epoch 1670), train_loss = 1.770, time/batch = 0.303\n",
      "28500/68000 (epoch 1676), train_loss = 1.782, time/batch = 0.305\n",
      "28600/68000 (epoch 1682), train_loss = 1.784, time/batch = 0.303\n",
      "28700/68000 (epoch 1688), train_loss = 1.796, time/batch = 0.307\n",
      "28800/68000 (epoch 1694), train_loss = 1.795, time/batch = 0.307\n",
      "28900/68000 (epoch 1700), train_loss = 1.806, time/batch = 0.303\n",
      "29000/68000 (epoch 1705), train_loss = 1.788, time/batch = 0.302\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "29100/68000 (epoch 1711), train_loss = 1.784, time/batch = 0.306\n",
      "29200/68000 (epoch 1717), train_loss = 1.780, time/batch = 0.306\n",
      "29300/68000 (epoch 1723), train_loss = 1.785, time/batch = 0.305\n",
      "29400/68000 (epoch 1729), train_loss = 1.780, time/batch = 0.305\n",
      "29500/68000 (epoch 1735), train_loss = 1.784, time/batch = 0.304\n",
      "29600/68000 (epoch 1741), train_loss = 1.774, time/batch = 0.306\n",
      "29700/68000 (epoch 1747), train_loss = 1.781, time/batch = 0.306\n",
      "29800/68000 (epoch 1752), train_loss = 1.778, time/batch = 0.304\n",
      "29900/68000 (epoch 1758), train_loss = 1.778, time/batch = 0.303\n",
      "30000/68000 (epoch 1764), train_loss = 1.775, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "30100/68000 (epoch 1770), train_loss = 1.769, time/batch = 0.304\n",
      "30200/68000 (epoch 1776), train_loss = 1.767, time/batch = 0.302\n",
      "30300/68000 (epoch 1782), train_loss = 1.775, time/batch = 0.303\n",
      "30400/68000 (epoch 1788), train_loss = 1.789, time/batch = 0.307\n",
      "30500/68000 (epoch 1794), train_loss = 1.796, time/batch = 0.305\n",
      "30600/68000 (epoch 1800), train_loss = 1.811, time/batch = 0.303\n",
      "30700/68000 (epoch 1805), train_loss = 1.783, time/batch = 0.305\n",
      "30800/68000 (epoch 1811), train_loss = 1.783, time/batch = 0.304\n",
      "30900/68000 (epoch 1817), train_loss = 1.782, time/batch = 0.303\n",
      "31000/68000 (epoch 1823), train_loss = 1.772, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "31100/68000 (epoch 1829), train_loss = 1.785, time/batch = 0.306\n",
      "31200/68000 (epoch 1835), train_loss = 1.774, time/batch = 0.306\n",
      "31300/68000 (epoch 1841), train_loss = 1.770, time/batch = 0.305\n",
      "31400/68000 (epoch 1847), train_loss = 1.770, time/batch = 0.306\n",
      "31500/68000 (epoch 1852), train_loss = 1.775, time/batch = 0.305\n",
      "31600/68000 (epoch 1858), train_loss = 1.780, time/batch = 0.305\n",
      "31700/68000 (epoch 1864), train_loss = 1.782, time/batch = 0.303\n",
      "31800/68000 (epoch 1870), train_loss = 1.764, time/batch = 0.305\n",
      "31900/68000 (epoch 1876), train_loss = 1.764, time/batch = 0.304\n",
      "32000/68000 (epoch 1882), train_loss = 1.769, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "32100/68000 (epoch 1888), train_loss = 1.791, time/batch = 0.304\n",
      "32200/68000 (epoch 1894), train_loss = 1.782, time/batch = 0.302\n",
      "32300/68000 (epoch 1900), train_loss = 1.796, time/batch = 0.307\n",
      "32400/68000 (epoch 1905), train_loss = 1.781, time/batch = 0.314\n",
      "32500/68000 (epoch 1911), train_loss = 1.777, time/batch = 0.305\n",
      "32600/68000 (epoch 1917), train_loss = 1.768, time/batch = 0.305\n",
      "32700/68000 (epoch 1923), train_loss = 1.767, time/batch = 0.308\n",
      "32800/68000 (epoch 1929), train_loss = 1.778, time/batch = 0.306\n",
      "32900/68000 (epoch 1935), train_loss = 1.790, time/batch = 0.305\n",
      "33000/68000 (epoch 1941), train_loss = 1.776, time/batch = 0.302\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "33100/68000 (epoch 1947), train_loss = 1.775, time/batch = 0.307\n",
      "33200/68000 (epoch 1952), train_loss = 1.773, time/batch = 0.303\n",
      "33300/68000 (epoch 1958), train_loss = 1.776, time/batch = 0.304\n",
      "33400/68000 (epoch 1964), train_loss = 1.764, time/batch = 0.302\n",
      "33500/68000 (epoch 1970), train_loss = 1.760, time/batch = 0.308\n",
      "33600/68000 (epoch 1976), train_loss = 1.754, time/batch = 0.305\n",
      "33700/68000 (epoch 1982), train_loss = 1.759, time/batch = 0.306\n",
      "33800/68000 (epoch 1988), train_loss = 1.774, time/batch = 0.305\n",
      "33900/68000 (epoch 1994), train_loss = 1.784, time/batch = 0.305\n",
      "34000/68000 (epoch 2000), train_loss = 1.794, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "34100/68000 (epoch 2005), train_loss = 1.772, time/batch = 0.306\n",
      "34200/68000 (epoch 2011), train_loss = 1.791, time/batch = 0.311\n",
      "34300/68000 (epoch 2017), train_loss = 1.768, time/batch = 0.305\n",
      "34400/68000 (epoch 2023), train_loss = 1.769, time/batch = 0.309\n",
      "34500/68000 (epoch 2029), train_loss = 1.767, time/batch = 0.305\n",
      "34600/68000 (epoch 2035), train_loss = 1.773, time/batch = 0.307\n",
      "34700/68000 (epoch 2041), train_loss = 1.756, time/batch = 0.303\n",
      "34800/68000 (epoch 2047), train_loss = 1.764, time/batch = 0.307\n",
      "34900/68000 (epoch 2052), train_loss = 1.771, time/batch = 0.304\n",
      "35000/68000 (epoch 2058), train_loss = 1.769, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "35100/68000 (epoch 2064), train_loss = 1.763, time/batch = 0.307\n",
      "35200/68000 (epoch 2070), train_loss = 1.756, time/batch = 0.306\n",
      "35300/68000 (epoch 2076), train_loss = 1.753, time/batch = 0.305\n",
      "35400/68000 (epoch 2082), train_loss = 1.761, time/batch = 0.307\n",
      "35500/68000 (epoch 2088), train_loss = 1.786, time/batch = 0.306\n",
      "35600/68000 (epoch 2094), train_loss = 1.774, time/batch = 0.305\n",
      "35700/68000 (epoch 2100), train_loss = 1.805, time/batch = 0.302\n",
      "35800/68000 (epoch 2105), train_loss = 1.774, time/batch = 0.304\n",
      "35900/68000 (epoch 2111), train_loss = 1.768, time/batch = 0.303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000/68000 (epoch 2117), train_loss = 1.766, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "36100/68000 (epoch 2123), train_loss = 1.756, time/batch = 0.307\n",
      "36200/68000 (epoch 2129), train_loss = 1.765, time/batch = 0.302\n",
      "36300/68000 (epoch 2135), train_loss = 1.768, time/batch = 0.306\n",
      "36400/68000 (epoch 2141), train_loss = 1.755, time/batch = 0.304\n",
      "36500/68000 (epoch 2147), train_loss = 1.761, time/batch = 0.305\n",
      "36600/68000 (epoch 2152), train_loss = 1.764, time/batch = 0.305\n",
      "36700/68000 (epoch 2158), train_loss = 1.760, time/batch = 0.302\n",
      "36800/68000 (epoch 2164), train_loss = 1.765, time/batch = 0.304\n",
      "36900/68000 (epoch 2170), train_loss = 1.745, time/batch = 0.306\n",
      "37000/68000 (epoch 2176), train_loss = 1.757, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "37100/68000 (epoch 2182), train_loss = 1.763, time/batch = 0.308\n",
      "37200/68000 (epoch 2188), train_loss = 1.775, time/batch = 0.307\n",
      "37300/68000 (epoch 2194), train_loss = 1.775, time/batch = 0.306\n",
      "37400/68000 (epoch 2200), train_loss = 1.794, time/batch = 0.302\n",
      "37500/68000 (epoch 2205), train_loss = 1.764, time/batch = 0.307\n",
      "37600/68000 (epoch 2211), train_loss = 1.767, time/batch = 0.302\n",
      "37700/68000 (epoch 2217), train_loss = 1.769, time/batch = 0.310\n",
      "37800/68000 (epoch 2223), train_loss = 1.755, time/batch = 0.303\n",
      "37900/68000 (epoch 2229), train_loss = 1.757, time/batch = 0.308\n",
      "38000/68000 (epoch 2235), train_loss = 1.769, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "38100/68000 (epoch 2241), train_loss = 1.759, time/batch = 0.304\n",
      "38200/68000 (epoch 2247), train_loss = 1.755, time/batch = 0.306\n",
      "38300/68000 (epoch 2252), train_loss = 1.763, time/batch = 0.307\n",
      "38400/68000 (epoch 2258), train_loss = 1.765, time/batch = 0.303\n",
      "38500/68000 (epoch 2264), train_loss = 1.762, time/batch = 0.307\n",
      "38600/68000 (epoch 2270), train_loss = 1.747, time/batch = 0.304\n",
      "38700/68000 (epoch 2276), train_loss = 1.746, time/batch = 0.305\n",
      "38800/68000 (epoch 2282), train_loss = 1.755, time/batch = 0.307\n",
      "38900/68000 (epoch 2288), train_loss = 1.777, time/batch = 0.307\n",
      "39000/68000 (epoch 2294), train_loss = 1.774, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "39100/68000 (epoch 2300), train_loss = 1.783, time/batch = 0.305\n",
      "39200/68000 (epoch 2305), train_loss = 1.769, time/batch = 0.304\n",
      "39300/68000 (epoch 2311), train_loss = 1.758, time/batch = 0.305\n",
      "39400/68000 (epoch 2317), train_loss = 1.765, time/batch = 0.305\n",
      "39500/68000 (epoch 2323), train_loss = 1.751, time/batch = 0.304\n",
      "39600/68000 (epoch 2329), train_loss = 1.754, time/batch = 0.306\n",
      "39700/68000 (epoch 2335), train_loss = 1.759, time/batch = 0.307\n",
      "39800/68000 (epoch 2341), train_loss = 1.755, time/batch = 0.303\n",
      "39900/68000 (epoch 2347), train_loss = 1.757, time/batch = 0.307\n",
      "40000/68000 (epoch 2352), train_loss = 1.755, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "40100/68000 (epoch 2358), train_loss = 1.761, time/batch = 0.304\n",
      "40200/68000 (epoch 2364), train_loss = 1.751, time/batch = 0.306\n",
      "40300/68000 (epoch 2370), train_loss = 1.744, time/batch = 0.301\n",
      "40400/68000 (epoch 2376), train_loss = 1.751, time/batch = 0.304\n",
      "40500/68000 (epoch 2382), train_loss = 1.745, time/batch = 0.306\n",
      "40600/68000 (epoch 2388), train_loss = 1.771, time/batch = 0.304\n",
      "40700/68000 (epoch 2394), train_loss = 1.762, time/batch = 0.306\n",
      "40800/68000 (epoch 2400), train_loss = 1.783, time/batch = 0.303\n",
      "40900/68000 (epoch 2405), train_loss = 1.756, time/batch = 0.303\n",
      "41000/68000 (epoch 2411), train_loss = 1.757, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "41100/68000 (epoch 2417), train_loss = 1.749, time/batch = 0.305\n",
      "41200/68000 (epoch 2423), train_loss = 1.751, time/batch = 0.308\n",
      "41300/68000 (epoch 2429), train_loss = 1.750, time/batch = 0.310\n",
      "41400/68000 (epoch 2435), train_loss = 1.760, time/batch = 0.303\n",
      "41500/68000 (epoch 2441), train_loss = 1.747, time/batch = 0.305\n",
      "41600/68000 (epoch 2447), train_loss = 1.753, time/batch = 0.304\n",
      "41700/68000 (epoch 2452), train_loss = 1.764, time/batch = 0.303\n",
      "41800/68000 (epoch 2458), train_loss = 1.753, time/batch = 0.303\n",
      "41900/68000 (epoch 2464), train_loss = 1.756, time/batch = 0.307\n",
      "42000/68000 (epoch 2470), train_loss = 1.737, time/batch = 0.309\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "42100/68000 (epoch 2476), train_loss = 1.732, time/batch = 0.305\n",
      "42200/68000 (epoch 2482), train_loss = 1.747, time/batch = 0.304\n",
      "42300/68000 (epoch 2488), train_loss = 1.761, time/batch = 0.305\n",
      "42400/68000 (epoch 2494), train_loss = 1.760, time/batch = 0.307\n",
      "42500/68000 (epoch 2500), train_loss = 1.777, time/batch = 0.307\n",
      "42600/68000 (epoch 2505), train_loss = 1.758, time/batch = 0.303\n",
      "42700/68000 (epoch 2511), train_loss = 1.757, time/batch = 0.305\n",
      "42800/68000 (epoch 2517), train_loss = 1.748, time/batch = 0.308\n",
      "42900/68000 (epoch 2523), train_loss = 1.742, time/batch = 0.303\n",
      "43000/68000 (epoch 2529), train_loss = 1.750, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "43100/68000 (epoch 2535), train_loss = 1.751, time/batch = 0.307\n",
      "43200/68000 (epoch 2541), train_loss = 1.747, time/batch = 0.306\n",
      "43300/68000 (epoch 2547), train_loss = 1.751, time/batch = 0.304\n",
      "43400/68000 (epoch 2552), train_loss = 1.763, time/batch = 0.306\n",
      "43500/68000 (epoch 2558), train_loss = 1.752, time/batch = 0.306\n",
      "43600/68000 (epoch 2564), train_loss = 1.753, time/batch = 0.306\n",
      "43700/68000 (epoch 2570), train_loss = 1.733, time/batch = 0.307\n",
      "43800/68000 (epoch 2576), train_loss = 1.734, time/batch = 0.303\n",
      "43900/68000 (epoch 2582), train_loss = 1.741, time/batch = 0.303\n",
      "44000/68000 (epoch 2588), train_loss = 1.760, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "44100/68000 (epoch 2594), train_loss = 1.764, time/batch = 0.302\n",
      "44200/68000 (epoch 2600), train_loss = 1.773, time/batch = 0.306\n",
      "44300/68000 (epoch 2605), train_loss = 1.750, time/batch = 0.308\n",
      "44400/68000 (epoch 2611), train_loss = 1.750, time/batch = 0.304\n",
      "44500/68000 (epoch 2617), train_loss = 1.744, time/batch = 0.306\n",
      "44600/68000 (epoch 2623), train_loss = 1.745, time/batch = 0.305\n",
      "44700/68000 (epoch 2629), train_loss = 1.742, time/batch = 0.306\n",
      "44800/68000 (epoch 2635), train_loss = 1.742, time/batch = 0.302\n",
      "44900/68000 (epoch 2641), train_loss = 1.742, time/batch = 0.307\n",
      "45000/68000 (epoch 2647), train_loss = 1.744, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "45100/68000 (epoch 2652), train_loss = 1.750, time/batch = 0.307\n",
      "45200/68000 (epoch 2658), train_loss = 1.740, time/batch = 0.304\n",
      "45300/68000 (epoch 2664), train_loss = 1.743, time/batch = 0.307\n",
      "45400/68000 (epoch 2670), train_loss = 1.730, time/batch = 0.303\n",
      "45500/68000 (epoch 2676), train_loss = 1.736, time/batch = 0.307\n",
      "45600/68000 (epoch 2682), train_loss = 1.733, time/batch = 0.306\n",
      "45700/68000 (epoch 2688), train_loss = 1.760, time/batch = 0.303\n",
      "45800/68000 (epoch 2694), train_loss = 1.754, time/batch = 0.304\n",
      "45900/68000 (epoch 2700), train_loss = 1.768, time/batch = 0.302\n",
      "46000/68000 (epoch 2705), train_loss = 1.752, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "46100/68000 (epoch 2711), train_loss = 1.749, time/batch = 0.306\n",
      "46200/68000 (epoch 2717), train_loss = 1.751, time/batch = 0.307\n",
      "46300/68000 (epoch 2723), train_loss = 1.742, time/batch = 0.308\n",
      "46400/68000 (epoch 2729), train_loss = 1.740, time/batch = 0.307\n",
      "46500/68000 (epoch 2735), train_loss = 1.740, time/batch = 0.305\n",
      "46600/68000 (epoch 2741), train_loss = 1.744, time/batch = 0.309\n",
      "46700/68000 (epoch 2747), train_loss = 1.741, time/batch = 0.304\n",
      "46800/68000 (epoch 2752), train_loss = 1.745, time/batch = 0.303\n",
      "46900/68000 (epoch 2758), train_loss = 1.746, time/batch = 0.303\n",
      "47000/68000 (epoch 2764), train_loss = 1.749, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "47100/68000 (epoch 2770), train_loss = 1.735, time/batch = 0.306\n",
      "47200/68000 (epoch 2776), train_loss = 1.737, time/batch = 0.310\n",
      "47300/68000 (epoch 2782), train_loss = 1.743, time/batch = 0.306\n",
      "47400/68000 (epoch 2788), train_loss = 1.751, time/batch = 0.305\n",
      "47500/68000 (epoch 2794), train_loss = 1.762, time/batch = 0.306\n",
      "47600/68000 (epoch 2800), train_loss = 1.765, time/batch = 0.303\n",
      "47700/68000 (epoch 2805), train_loss = 1.752, time/batch = 0.308\n",
      "47800/68000 (epoch 2811), train_loss = 1.739, time/batch = 0.304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47900/68000 (epoch 2817), train_loss = 1.741, time/batch = 0.306\n",
      "48000/68000 (epoch 2823), train_loss = 1.734, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "48100/68000 (epoch 2829), train_loss = 1.736, time/batch = 0.307\n",
      "48200/68000 (epoch 2835), train_loss = 1.743, time/batch = 0.304\n",
      "48300/68000 (epoch 2841), train_loss = 1.739, time/batch = 0.305\n",
      "48400/68000 (epoch 2847), train_loss = 1.742, time/batch = 0.304\n",
      "48500/68000 (epoch 2852), train_loss = 1.735, time/batch = 0.304\n",
      "48600/68000 (epoch 2858), train_loss = 1.740, time/batch = 0.307\n",
      "48700/68000 (epoch 2864), train_loss = 1.741, time/batch = 0.304\n",
      "48800/68000 (epoch 2870), train_loss = 1.729, time/batch = 0.306\n",
      "48900/68000 (epoch 2876), train_loss = 1.727, time/batch = 0.306\n",
      "49000/68000 (epoch 2882), train_loss = 1.744, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "49100/68000 (epoch 2888), train_loss = 1.758, time/batch = 0.306\n",
      "49200/68000 (epoch 2894), train_loss = 1.753, time/batch = 0.305\n",
      "49300/68000 (epoch 2900), train_loss = 1.766, time/batch = 0.307\n",
      "49400/68000 (epoch 2905), train_loss = 1.749, time/batch = 0.304\n",
      "49500/68000 (epoch 2911), train_loss = 1.751, time/batch = 0.307\n",
      "49600/68000 (epoch 2917), train_loss = 1.738, time/batch = 0.306\n",
      "49700/68000 (epoch 2923), train_loss = 1.730, time/batch = 0.307\n",
      "49800/68000 (epoch 2929), train_loss = 1.740, time/batch = 0.307\n",
      "49900/68000 (epoch 2935), train_loss = 1.740, time/batch = 0.304\n",
      "50000/68000 (epoch 2941), train_loss = 1.729, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "50100/68000 (epoch 2947), train_loss = 1.742, time/batch = 0.305\n",
      "50200/68000 (epoch 2952), train_loss = 1.737, time/batch = 0.305\n",
      "50300/68000 (epoch 2958), train_loss = 1.740, time/batch = 0.303\n",
      "50400/68000 (epoch 2964), train_loss = 1.739, time/batch = 0.306\n",
      "50500/68000 (epoch 2970), train_loss = 1.734, time/batch = 0.303\n",
      "50600/68000 (epoch 2976), train_loss = 1.727, time/batch = 0.305\n",
      "50700/68000 (epoch 2982), train_loss = 1.729, time/batch = 0.304\n",
      "50800/68000 (epoch 2988), train_loss = 1.752, time/batch = 0.304\n",
      "50900/68000 (epoch 2994), train_loss = 1.751, time/batch = 0.308\n",
      "51000/68000 (epoch 3000), train_loss = 1.766, time/batch = 0.302\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "51100/68000 (epoch 3005), train_loss = 1.742, time/batch = 0.309\n",
      "51200/68000 (epoch 3011), train_loss = 1.739, time/batch = 0.303\n",
      "51300/68000 (epoch 3017), train_loss = 1.742, time/batch = 0.307\n",
      "51400/68000 (epoch 3023), train_loss = 1.729, time/batch = 0.308\n",
      "51500/68000 (epoch 3029), train_loss = 1.731, time/batch = 0.306\n",
      "51600/68000 (epoch 3035), train_loss = 1.737, time/batch = 0.302\n",
      "51700/68000 (epoch 3041), train_loss = 1.732, time/batch = 0.302\n",
      "51800/68000 (epoch 3047), train_loss = 1.734, time/batch = 0.304\n",
      "51900/68000 (epoch 3052), train_loss = 1.732, time/batch = 0.303\n",
      "52000/68000 (epoch 3058), train_loss = 1.740, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "52100/68000 (epoch 3064), train_loss = 1.735, time/batch = 0.307\n",
      "52200/68000 (epoch 3070), train_loss = 1.728, time/batch = 0.308\n",
      "52300/68000 (epoch 3076), train_loss = 1.728, time/batch = 0.305\n",
      "52400/68000 (epoch 3082), train_loss = 1.736, time/batch = 0.304\n",
      "52500/68000 (epoch 3088), train_loss = 1.748, time/batch = 0.302\n",
      "52600/68000 (epoch 3094), train_loss = 1.753, time/batch = 0.305\n",
      "52700/68000 (epoch 3100), train_loss = 1.757, time/batch = 0.301\n",
      "52800/68000 (epoch 3105), train_loss = 1.739, time/batch = 0.304\n",
      "52900/68000 (epoch 3111), train_loss = 1.740, time/batch = 0.305\n",
      "53000/68000 (epoch 3117), train_loss = 1.729, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "53100/68000 (epoch 3123), train_loss = 1.732, time/batch = 0.303\n",
      "53200/68000 (epoch 3129), train_loss = 1.737, time/batch = 0.305\n",
      "53300/68000 (epoch 3135), train_loss = 1.729, time/batch = 0.309\n",
      "53400/68000 (epoch 3141), train_loss = 1.728, time/batch = 0.306\n",
      "53500/68000 (epoch 3147), train_loss = 1.740, time/batch = 0.306\n",
      "53600/68000 (epoch 3152), train_loss = 1.735, time/batch = 0.304\n",
      "53700/68000 (epoch 3158), train_loss = 1.742, time/batch = 0.303\n",
      "53800/68000 (epoch 3164), train_loss = 1.748, time/batch = 0.306\n",
      "53900/68000 (epoch 3170), train_loss = 1.721, time/batch = 0.310\n",
      "54000/68000 (epoch 3176), train_loss = 1.720, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "54100/68000 (epoch 3182), train_loss = 1.731, time/batch = 0.309\n",
      "54200/68000 (epoch 3188), train_loss = 1.742, time/batch = 0.303\n",
      "54300/68000 (epoch 3194), train_loss = 1.740, time/batch = 0.308\n",
      "54400/68000 (epoch 3200), train_loss = 1.756, time/batch = 0.303\n",
      "54500/68000 (epoch 3205), train_loss = 1.750, time/batch = 0.305\n",
      "54600/68000 (epoch 3211), train_loss = 1.742, time/batch = 0.306\n",
      "54700/68000 (epoch 3217), train_loss = 1.735, time/batch = 0.306\n",
      "54800/68000 (epoch 3223), train_loss = 1.734, time/batch = 0.305\n",
      "54900/68000 (epoch 3229), train_loss = 1.729, time/batch = 0.307\n",
      "55000/68000 (epoch 3235), train_loss = 1.730, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "55100/68000 (epoch 3241), train_loss = 1.720, time/batch = 0.305\n",
      "55200/68000 (epoch 3247), train_loss = 1.728, time/batch = 0.302\n",
      "55300/68000 (epoch 3252), train_loss = 1.737, time/batch = 0.307\n",
      "55400/68000 (epoch 3258), train_loss = 1.736, time/batch = 0.305\n",
      "55500/68000 (epoch 3264), train_loss = 1.724, time/batch = 0.306\n",
      "55600/68000 (epoch 3270), train_loss = 1.718, time/batch = 0.307\n",
      "55700/68000 (epoch 3276), train_loss = 1.724, time/batch = 0.307\n",
      "55800/68000 (epoch 3282), train_loss = 1.733, time/batch = 0.306\n",
      "55900/68000 (epoch 3288), train_loss = 1.750, time/batch = 0.304\n",
      "56000/68000 (epoch 3294), train_loss = 1.743, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "56100/68000 (epoch 3300), train_loss = 1.749, time/batch = 0.303\n",
      "56200/68000 (epoch 3305), train_loss = 1.737, time/batch = 0.303\n",
      "56300/68000 (epoch 3311), train_loss = 1.726, time/batch = 0.304\n",
      "56400/68000 (epoch 3317), train_loss = 1.729, time/batch = 0.303\n",
      "56500/68000 (epoch 3323), train_loss = 1.719, time/batch = 0.304\n",
      "56600/68000 (epoch 3329), train_loss = 1.715, time/batch = 0.308\n",
      "56700/68000 (epoch 3335), train_loss = 1.734, time/batch = 0.306\n",
      "56800/68000 (epoch 3341), train_loss = 1.719, time/batch = 0.302\n",
      "56900/68000 (epoch 3347), train_loss = 1.729, time/batch = 0.306\n",
      "57000/68000 (epoch 3352), train_loss = 1.727, time/batch = 0.308\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "57100/68000 (epoch 3358), train_loss = 1.731, time/batch = 0.303\n",
      "57200/68000 (epoch 3364), train_loss = 1.725, time/batch = 0.306\n",
      "57300/68000 (epoch 3370), train_loss = 1.712, time/batch = 0.308\n",
      "57400/68000 (epoch 3376), train_loss = 1.715, time/batch = 0.308\n",
      "57500/68000 (epoch 3382), train_loss = 1.725, time/batch = 0.308\n",
      "57600/68000 (epoch 3388), train_loss = 1.741, time/batch = 0.307\n",
      "57700/68000 (epoch 3394), train_loss = 1.736, time/batch = 0.303\n",
      "57800/68000 (epoch 3400), train_loss = 1.748, time/batch = 0.302\n",
      "57900/68000 (epoch 3405), train_loss = 1.725, time/batch = 0.302\n",
      "58000/68000 (epoch 3411), train_loss = 1.731, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "58100/68000 (epoch 3417), train_loss = 1.723, time/batch = 0.306\n",
      "58200/68000 (epoch 3423), train_loss = 1.723, time/batch = 0.305\n",
      "58300/68000 (epoch 3429), train_loss = 1.720, time/batch = 0.305\n",
      "58400/68000 (epoch 3435), train_loss = 1.731, time/batch = 0.305\n",
      "58500/68000 (epoch 3441), train_loss = 1.723, time/batch = 0.307\n",
      "58600/68000 (epoch 3447), train_loss = 1.723, time/batch = 0.306\n",
      "58700/68000 (epoch 3452), train_loss = 1.725, time/batch = 0.306\n",
      "58800/68000 (epoch 3458), train_loss = 1.728, time/batch = 0.309\n",
      "58900/68000 (epoch 3464), train_loss = 1.720, time/batch = 0.306\n",
      "59000/68000 (epoch 3470), train_loss = 1.721, time/batch = 0.304\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "59100/68000 (epoch 3476), train_loss = 1.708, time/batch = 0.307\n",
      "59200/68000 (epoch 3482), train_loss = 1.713, time/batch = 0.310\n",
      "59300/68000 (epoch 3488), train_loss = 1.730, time/batch = 0.305\n",
      "59400/68000 (epoch 3494), train_loss = 1.752, time/batch = 0.305\n",
      "59500/68000 (epoch 3500), train_loss = 1.751, time/batch = 0.308\n",
      "59600/68000 (epoch 3505), train_loss = 1.726, time/batch = 0.305\n",
      "59700/68000 (epoch 3511), train_loss = 1.726, time/batch = 0.308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59800/68000 (epoch 3517), train_loss = 1.722, time/batch = 0.307\n",
      "59900/68000 (epoch 3523), train_loss = 1.727, time/batch = 0.306\n",
      "60000/68000 (epoch 3529), train_loss = 1.727, time/batch = 0.307\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "60100/68000 (epoch 3535), train_loss = 1.725, time/batch = 0.307\n",
      "60200/68000 (epoch 3541), train_loss = 1.717, time/batch = 0.302\n",
      "60300/68000 (epoch 3547), train_loss = 1.718, time/batch = 0.303\n",
      "60400/68000 (epoch 3552), train_loss = 1.728, time/batch = 0.307\n",
      "60500/68000 (epoch 3558), train_loss = 1.721, time/batch = 0.304\n",
      "60600/68000 (epoch 3564), train_loss = 1.722, time/batch = 0.302\n",
      "60700/68000 (epoch 3570), train_loss = 1.711, time/batch = 0.303\n",
      "60800/68000 (epoch 3576), train_loss = 1.721, time/batch = 0.304\n",
      "60900/68000 (epoch 3582), train_loss = 1.717, time/batch = 0.302\n",
      "61000/68000 (epoch 3588), train_loss = 1.736, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "61100/68000 (epoch 3594), train_loss = 1.729, time/batch = 0.306\n",
      "61200/68000 (epoch 3600), train_loss = 1.741, time/batch = 0.302\n",
      "61300/68000 (epoch 3605), train_loss = 1.726, time/batch = 0.306\n",
      "61400/68000 (epoch 3611), train_loss = 1.730, time/batch = 0.307\n",
      "61500/68000 (epoch 3617), train_loss = 1.727, time/batch = 0.303\n",
      "61600/68000 (epoch 3623), train_loss = 1.714, time/batch = 0.306\n",
      "61700/68000 (epoch 3629), train_loss = 1.723, time/batch = 0.306\n",
      "61800/68000 (epoch 3635), train_loss = 1.722, time/batch = 0.304\n",
      "61900/68000 (epoch 3641), train_loss = 1.714, time/batch = 0.302\n",
      "62000/68000 (epoch 3647), train_loss = 1.718, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "62100/68000 (epoch 3652), train_loss = 1.720, time/batch = 0.307\n",
      "62200/68000 (epoch 3658), train_loss = 1.715, time/batch = 0.302\n",
      "62300/68000 (epoch 3664), train_loss = 1.729, time/batch = 0.306\n",
      "62400/68000 (epoch 3670), train_loss = 1.711, time/batch = 0.302\n",
      "62500/68000 (epoch 3676), train_loss = 1.707, time/batch = 0.307\n",
      "62600/68000 (epoch 3682), train_loss = 1.723, time/batch = 0.303\n",
      "62700/68000 (epoch 3688), train_loss = 1.735, time/batch = 0.303\n",
      "62800/68000 (epoch 3694), train_loss = 1.743, time/batch = 0.306\n",
      "62900/68000 (epoch 3700), train_loss = 1.746, time/batch = 0.302\n",
      "63000/68000 (epoch 3705), train_loss = 1.727, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "63100/68000 (epoch 3711), train_loss = 1.728, time/batch = 0.303\n",
      "63200/68000 (epoch 3717), train_loss = 1.719, time/batch = 0.305\n",
      "63300/68000 (epoch 3723), train_loss = 1.714, time/batch = 0.307\n",
      "63400/68000 (epoch 3729), train_loss = 1.717, time/batch = 0.304\n",
      "63500/68000 (epoch 3735), train_loss = 1.728, time/batch = 0.302\n",
      "63600/68000 (epoch 3741), train_loss = 1.719, time/batch = 0.308\n",
      "63700/68000 (epoch 3747), train_loss = 1.722, time/batch = 0.306\n",
      "63800/68000 (epoch 3752), train_loss = 1.715, time/batch = 0.305\n",
      "63900/68000 (epoch 3758), train_loss = 1.721, time/batch = 0.305\n",
      "64000/68000 (epoch 3764), train_loss = 1.716, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "64100/68000 (epoch 3770), train_loss = 1.706, time/batch = 0.308\n",
      "64200/68000 (epoch 3776), train_loss = 1.705, time/batch = 0.303\n",
      "64300/68000 (epoch 3782), train_loss = 1.712, time/batch = 0.304\n",
      "64400/68000 (epoch 3788), train_loss = 1.733, time/batch = 0.306\n",
      "64500/68000 (epoch 3794), train_loss = 1.734, time/batch = 0.304\n",
      "64600/68000 (epoch 3800), train_loss = 1.745, time/batch = 0.308\n",
      "64700/68000 (epoch 3805), train_loss = 1.726, time/batch = 0.306\n",
      "64800/68000 (epoch 3811), train_loss = 1.724, time/batch = 0.303\n",
      "64900/68000 (epoch 3817), train_loss = 1.713, time/batch = 0.304\n",
      "65000/68000 (epoch 3823), train_loss = 1.705, time/batch = 0.303\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "65100/68000 (epoch 3829), train_loss = 1.728, time/batch = 0.310\n",
      "65200/68000 (epoch 3835), train_loss = 1.730, time/batch = 0.304\n",
      "65300/68000 (epoch 3841), train_loss = 1.711, time/batch = 0.305\n",
      "65400/68000 (epoch 3847), train_loss = 1.718, time/batch = 0.307\n",
      "65500/68000 (epoch 3852), train_loss = 1.719, time/batch = 0.307\n",
      "65600/68000 (epoch 3858), train_loss = 1.720, time/batch = 0.306\n",
      "65700/68000 (epoch 3864), train_loss = 1.714, time/batch = 0.306\n",
      "65800/68000 (epoch 3870), train_loss = 1.696, time/batch = 0.306\n",
      "65900/68000 (epoch 3876), train_loss = 1.701, time/batch = 0.307\n",
      "66000/68000 (epoch 3882), train_loss = 1.715, time/batch = 0.306\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "66100/68000 (epoch 3888), train_loss = 1.728, time/batch = 0.308\n",
      "66200/68000 (epoch 3894), train_loss = 1.743, time/batch = 0.306\n",
      "66300/68000 (epoch 3900), train_loss = 1.742, time/batch = 0.302\n",
      "66400/68000 (epoch 3905), train_loss = 1.728, time/batch = 0.306\n",
      "66500/68000 (epoch 3911), train_loss = 1.727, time/batch = 0.306\n",
      "66600/68000 (epoch 3917), train_loss = 1.722, time/batch = 0.304\n",
      "66700/68000 (epoch 3923), train_loss = 1.712, time/batch = 0.306\n",
      "66800/68000 (epoch 3929), train_loss = 1.721, time/batch = 0.303\n",
      "66900/68000 (epoch 3935), train_loss = 1.722, time/batch = 0.302\n",
      "67000/68000 (epoch 3941), train_loss = 1.708, time/batch = 0.305\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "67100/68000 (epoch 3947), train_loss = 1.709, time/batch = 0.302\n",
      "67200/68000 (epoch 3952), train_loss = 1.715, time/batch = 0.305\n",
      "67300/68000 (epoch 3958), train_loss = 1.717, time/batch = 0.302\n",
      "67400/68000 (epoch 3964), train_loss = 1.718, time/batch = 0.309\n",
      "67500/68000 (epoch 3970), train_loss = 1.705, time/batch = 0.303\n",
      "67600/68000 (epoch 3976), train_loss = 1.697, time/batch = 0.306\n",
      "67700/68000 (epoch 3982), train_loss = 1.707, time/batch = 0.303\n",
      "67800/68000 (epoch 3988), train_loss = 1.723, time/batch = 0.305\n",
      "67900/68000 (epoch 3994), train_loss = 1.727, time/batch = 0.302\n",
      "model saved to models_char_rnn/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# let's train!\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars_vocab.pkl                       model.ckpt-33999.data-00000-of-00001\r\n",
      "checkpoint                            model.ckpt-33999.index\r\n",
      "config.pkl                            model.ckpt-33999.meta\r\n",
      "model.ckpt-16999.data-00000-of-00001  model.ckpt-67999.data-00000-of-00001\r\n",
      "model.ckpt-16999.index                model.ckpt-67999.index\r\n",
      "model.ckpt-16999.meta                 model.ckpt-67999.meta\r\n"
     ]
    }
   ],
   "source": [
    "# we're done with the model. the weights are now safe inside our storage\n",
    "%ls {args.save_dir}\n",
    "\n",
    "# so begone all the ops, graphs & variables!\n",
    "# you may ask, why is this line needed? try commenting out the line and see what happens later in the sampling stage\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalulation\n",
    "\n",
    "<font color=red>**Your model could be evaluated without traning procedure,**</font> if you saved and loaded your model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sampling definition for evaluation phase\n",
    "# it uses the saved model and spit out some characters from the RNN model\n",
    "def sample_eval(args):\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    #Use most frequent char if no prime is given\n",
    "    if args.prime == '':\n",
    "        args.prime = chars[0]\n",
    "    tf.reset_default_graph()\n",
    "    model = Model(saved_args, training=False)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(str(model.sample(sess, chars, vocab, args.n, args.prime)),'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(n=500, prime='', save_dir='models_char_rnn')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing for sampling from the model\n",
    "parser_sample = argparse.ArgumentParser(\n",
    "                   formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser_sample.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='model directory to store checkpointed models')\n",
    "parser_sample.add_argument('-n', type=int, default=500,\n",
    "                    help='number of characters to sample')\n",
    "parser_sample.add_argument('--prime', type=text_type, default=u'',\n",
    "                    help='prime text')\n",
    "sys.argv = ['-f']\n",
    "args_sample = parser_sample.parse_args()\n",
    "args_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models_char_rnn/model.ckpt-67999\n",
      " his raee dombrian ghils,\n",
      "They cay.'\n",
      "a'd. I have that bott nxalase; you are sood now noben\n",
      "that momentisy till thou shalt be iser-y\n",
      "Fechire my wat.\n",
      "\n",
      "CLARENCE:\n",
      "Let them rogeet, werc, I beseech your ladins?\n",
      "Here falls the war: and menc and all an-other:\n",
      "Hath kung my bidth's bake in pakeet net\n",
      "bis, weichty Ip have thee voved and that.\n",
      "Pawigh'd somiciwe, I ponours flither.\n",
      "\n",
      "LELEUTIO:\n",
      "Tey ore necrt, friend, heir, 'sis wher will geet\n",
      "To thy blaatin lile shall break it anl.\n",
      "Thas Henk a triet and lamenta utf-8\n"
     ]
    }
   ],
   "source": [
    "# let's sample!\n",
    "sample_eval(args_sample)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Briefly summarize what & how you did, and why you did that way.\n",
    "This is also for checking yourself if you really learned something from this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>About the models and sampler codes, most of the codes are similar with the reference github codes. But it does not converge well enough to reduce the training loss. So it does not give us good enough sentences.</p>\n",
    "\n",
    "#### learning rate\n",
    "<p>After 4~5 epochs train loss is even increased during the traning phase. So we reduced the initial learning rate from 1e-2 to 1e-3 and slow the decaying rate by dividing it with the total number of epochs.</p>\n",
    "\n",
    "#### thin and wide rnn model\n",
    "<p>Within the memory capacity of the GPU, we tested various models from deep and narrow to thin and wide. The deep model means that it has 3 to 4 multi-layer rnn cells. And the wide means the hidden dimension of rnn cell. By the experiments, we conclude that thin and wide model is better. So we set the dimension of hidden to 638 and 1 layer rnn cell</p>\n",
    "\n",
    "#### high dropout rate\n",
    "<p>Our input/output keep prob is setted to 0.1 which is high dropout rate. Our stratage is that choose the complex model as possible within our GPU memory capacity, and get generalization effect with high dropout rate. And training as long as possible</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
